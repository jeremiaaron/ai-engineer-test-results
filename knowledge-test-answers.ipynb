{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d28b8a-0693-4eb4-8ca7-1b4ab8af190e",
   "metadata": {},
   "source": [
    "# Knowledge Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3451e-6f97-4254-9f50-086e5f748122",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19adf7b8-16f2-4a19-afd8-7f2d24bf813b",
   "metadata": {},
   "source": [
    "Question: Describe differences between REST API, MCP in the context of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481f252-1605-41b7-9eb6-26e2b857f701",
   "metadata": {},
   "source": [
    "Answer:\n",
    "REST API has existed since long ago, enabling communication and data exchange between different software applications with CRUD methods called GET, POST, PUT, PATCH, and DELETE. Right now, REST API is also important for AI agents because while AI agents may have a large amount of knowledge, to get reliable & relevant data or to perform actions, they need to interact with other applications. Interaction between applications is done via API, typically REST API. However, the problem is that there are so many REST APIs out there, each with its own schema, so it will be a hassle to configure the AI agents to interact with all of the different APIs.\n",
    "\n",
    "MCP (Model Context Protocol) solves this problem. Essentially, the AI agents have a component called MCP client which interacts with MCP servers. MCP servers contain tools (to perform action) and/or resources (to retrieve data). The MCP server also provides a structure and context so that the AI agents can understand what the MCP server capabilities are. In a sense, MCP does not replace REST API, but it optimizes how AI agents can interact with REST APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc6133-9533-4b2d-85ae-c5c313a4c747",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14931c-34b9-49e8-a7a7-da41fe238e0f",
   "metadata": {},
   "source": [
    "Question: How REST API, MCP, can improve the AI use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a56d50-d525-4dae-b453-42fc59519f81",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Simple use case example:\n",
    "We want to build an AI agent that helps people in easily checking and booking flights. It needs to retrieve information from all available airlines, let's say five airlines. Each airline provides an API to retrieve flight data and book a flight. The problem is that while the APIs seem similar, they have different schemas, field names (one may use \"origin_city\" but the other \"origin_loc\"), and authentication (different bearer tokens need to be generated for all APIs). To solve this, we engineer an MCP server that contains the information of all airline APIs and automate the generation of bearer tokens for each API. Finally, we create and describe a schema with field names, e.g., \"flight_date\", \"flight_time\", \"origin\", \"destination\", and \"airline\".\n",
    "\n",
    "Since we already have the MCP server that receives an input based on the configured schema and execute the APIs based on the input, all the AI agents need to do is understand the given schema and translate the user query into the schema. When a user queries \"check flight from jakarta to denpasar in december 2025\" to the AI agent, the MCP client of the agent will connect to the MCP server and find the tools provided by the MCP server. Since the user wants to check flights information, then the AI agent will choose the most relevant tool \"retrieve_flight_data\" and extract the required information from the user query. The MCP client will then call the \"retrieve_flight_data\" tool and pass all the extracted \"flight_date\", \"flight_time\", \"origin\", \"destination\", and \"airline\" information. The MCP server executes that request to the airlines API and returns the results back to the MCP client, and now the AI agent can inform the user about available flights from Jakarta to Denpasar in December 2025.\n",
    "\n",
    "REST APIs combined with MCP are now definitely the standard for powerful AI agents. By using REST API and MCP, we can reduce AI agents hallucinations since they are grounded with reliable data, and we can build powerful AI agents that could interact with other applications. The airlines use case is just an example, but there are so many other possibilities and applications. Some of my experience related to this is an AI agent I built that could interact with HRIS so employees can submit leave requests or check their personal data directly through the AI agent. Another one with similar concept is a built-in connector on n8n, which allows me to input data and track my expense just by chatting with an AI agent on Slack platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbf2e0-dd41-4d16-a6bd-0d1a58b391f8",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b3629-7c31-4df8-8a57-bd088d977690",
   "metadata": {},
   "source": [
    "Question: How do you ensure that your AI agent answers correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6acf3-472a-421c-bd97-5dbc20c2362a",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Based on my experience, there are 3 prominent things that will influence accuracy of AI agent answers: prompts, retrieval, and evaluation.\n",
    "\n",
    "- **Prompt engineering** is very important because the right structure, neat and complete examples, and clear rules and constraints could heavily affect the quality of AI agent outputs.\n",
    "- **Retrieval** is a must if we want the AI agent to answer grounded on reliable data. Prompt engineering is also important when combined with retrieval because we need to instruct the model how to answer based on the retrieved data or how to answer if there is no data.\n",
    "- Finally, we need a rigorous method of **evaluation**. A must have is definitely a logging system, and a typical approach for evaluation is by using a golden dataset of questions and the correct answers. By using a golden dataset, the responses of an AI agent can be evaluated by a human based on a ground truth data (or evaluated by another LLM, but in my opinion, human evaluation is the most reliable). Another approach I implemented for my AI agent is tracking \"unanswered\" questions, meaning all questions that couldn't be answered by the AI agent are flagged. Then, by human supervision, the model can be enriched with more data so that next time, it could answer the same questions if asked again. For AI agents that interact with other applications via REST API and/or MCP, then there needs to be a comprehensive user testing that covers all cases and edge cases. This is to ensure that the AI agents are already performing the correct actions and the system is reliable enough to be deployed into production (I assure you, it is NOT funny if the employee wants to submit medical check up request, but a leave request is submitted instead, so a human-in-the-loop elicitation process also needs to be considered as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf773e-3a4f-451d-b32b-dab29a11f0e8",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a96e1-71bb-463e-9f33-ea9c444be805",
   "metadata": {},
   "source": [
    "Question: Describe what can you do with Docker / Containerize environment in the context of AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f65f4af-5066-4ab4-a519-789f3f6c21ad",
   "metadata": {},
   "source": [
    "Answer: The advantages of using docker/containerization environment in AI is similar to its advantages for any other applications.\n",
    "- **The first advantage is isolation.**\n",
    "\n",
    "With isolation, we know for sure that there will be no library dependency conflicts across different applications because each application is now in its own container. With isolation also comes portability, meaning that it is easy to move Docker containers across different operating systems.\n",
    "\n",
    "- **The second advantage is simpler and faster CI/CD process.**\n",
    "\n",
    "By using docker containers, development is easier and the CI/CD also becomes simpler. For example, in my experience, by using Docker, I can easily integrate the CI/CD process with GitHub Actions, I just needed to create the docker setup configuration in the yml config file, and when I commit & push my code from server to GitHub, the GitHub Actions will just run based on the config file and run the docker container as instructed, the new code will automatically be deployed in my server. All of that was made possible by using Docker containers\n",
    "\n",
    "- **The third advantage is lightweight and scalability.**\n",
    "\n",
    "Before the existence of Docker containers, applications are usually deployed in different operating systems, so it might not be worth it for low-resource applications. With Docker containers, now multiple applications can just be deployed in different containers but still using the same underlying operating system, meaning that now all applications are maximizing the use of available resources in a server. If this is combined with a container orchestration tool like Kubernetes, then it will be much easier to scale up or down applications since Kubernetes can automatically distribute workloads for every container based on the resources it needs and how heavy the load is.\n",
    "\n",
    "The rising demand of AI applications at this moment is a perfect way to showcase how powerful Docker containers are. A huge number of AI application demands calls for faster application development and deployment. The development and deployment can also be very dynamic, meaning libraries and frameworks evolve faster than ever (take a look at LangChain & LangGraph, it has evolved quickly and is still evolving), new methods of AI development like new RAG techniques are consistently emerging, and the number of people using AI applications are also rising (assuming that we've built a useful AI application). The rapid, dynamic, and high-scale nature of AI development requires the benefits that come with using Docker containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aca047-3c49-499d-a1d0-3879d9db192f",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7aff2f-0f66-48da-9104-4530aadbe2d9",
   "metadata": {},
   "source": [
    "Question: How do you finetune the LLM model from raw ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fa061-c67c-4db9-9e3f-7965b0a6b271",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Finetuning the LLM model can mean re-training all the original model's parameters/weights entirely or freezing the original model's weights and adding new trainable weights, then only the new weights are finetuned. The former option is VERY expensive and time-consuming, so the latter option is the common method, and this is what's typically called the PEFT (Parameter-Efficient Fine-Tuning) method.\n",
    "\n",
    "Even with the original model's weights frozen, the number of new weights are still too much to be efficiently trained. To mitigate this, there are two techniques used in PEFT: LoRA and QLoRA. Both benefit from the \"rank\" hyperparameter (like their names, Low Rank Adaptation), but for QLoRA, it trains a quantized LLM instead (quantized LLM essentially means the LLM uses lower memory but still maintaining relatively decent performance). In PEFT, there are supervised finetuning and unsupervised fine tuning. I've only had experience on the supervised finetuning. For that, we need to curate a dataset of question and answer pairs. This dataset can be created by feeding a raw document text to a \"super\" LLM like Gemini and ask it to create question and answer pairs from the text. When the dataset is ready, we can use a framework like TRL by huggingface to finetune the LLM model. The hyperparameters are similar to deep learning model training (learning rate, epoch, etc.) because an LLM model is a subset of deep learning models. After finetuning, we can evaluate how the LLM answers about the dataset it has been trained on. \n",
    "\n",
    "The main challenge (based on my experience) is the resources needed, and by resources, I mean the cost of hardware and the amount of time required to finetune the LLM model. To finetune a model, we need GPUs with high VRAM and compute power. Typically, we would need at least 80GB A100 or H100 for finetuning a 7-13B model (also depending on the techniques used like quantization and/or mixed precision), and for larger models, we would need multiple models and it would be costly. Another approach is to rent a cloud GPU provider or take advantage of free compute units by Google Colab and Kaggle.\n",
    "\n",
    "It takes an effort and quite a lot of resources to finetune an LLM model. However, everything still depends on the use case. If we have a solid use case that could benefit a lot from LLM finetuning, e.g., better named entity recognition, optimized data extraction, and text classification, then it is a viable solution. LLM finetuning is powerful, but we just need to be mindful that the use case needs to be as solid as possible because finetuning an LLM model is a HUGE commitment of time and money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2663f-2d56-4dfb-83bf-52a14404ddad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
